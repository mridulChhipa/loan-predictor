{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b2ef14c-32fd-48bc-b53a-b2a16fbb9aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss=0.6657, Train Acc=0.5873\n",
      "Epoch 002: Loss=0.6484, Train Acc=0.6090\n",
      "Epoch 003: Loss=0.6332, Train Acc=0.6267\n",
      "Epoch 004: Loss=0.6198, Train Acc=0.6425\n",
      "Epoch 005: Loss=0.6078, Train Acc=0.6564\n",
      "Epoch 006: Loss=0.5969, Train Acc=0.6698\n",
      "Epoch 007: Loss=0.5869, Train Acc=0.6825\n",
      "Epoch 008: Loss=0.5776, Train Acc=0.6953\n",
      "Epoch 009: Loss=0.5689, Train Acc=0.7077\n",
      "Epoch 010: Loss=0.5606, Train Acc=0.7207\n",
      "Epoch 011: Loss=0.5528, Train Acc=0.7337\n",
      "Epoch 012: Loss=0.5453, Train Acc=0.7458\n",
      "Epoch 013: Loss=0.5380, Train Acc=0.7574\n",
      "Epoch 014: Loss=0.5310, Train Acc=0.7676\n",
      "Epoch 015: Loss=0.5243, Train Acc=0.7774\n",
      "Epoch 016: Loss=0.5178, Train Acc=0.7870\n",
      "Epoch 017: Loss=0.5115, Train Acc=0.7960\n",
      "Epoch 018: Loss=0.5055, Train Acc=0.8037\n",
      "Epoch 019: Loss=0.4998, Train Acc=0.8100\n",
      "Epoch 020: Loss=0.4943, Train Acc=0.8151\n",
      "Epoch 021: Loss=0.4890, Train Acc=0.8192\n",
      "Epoch 022: Loss=0.4839, Train Acc=0.8231\n",
      "Epoch 023: Loss=0.4791, Train Acc=0.8263\n",
      "Epoch 024: Loss=0.4744, Train Acc=0.8290\n",
      "Epoch 025: Loss=0.4698, Train Acc=0.8317\n",
      "Epoch 026: Loss=0.4654, Train Acc=0.8341\n",
      "Epoch 027: Loss=0.4612, Train Acc=0.8362\n",
      "Epoch 028: Loss=0.4571, Train Acc=0.8382\n",
      "Epoch 029: Loss=0.4531, Train Acc=0.8398\n",
      "Epoch 030: Loss=0.4492, Train Acc=0.8414\n",
      "Epoch 031: Loss=0.4455, Train Acc=0.8427\n",
      "Epoch 032: Loss=0.4419, Train Acc=0.8438\n",
      "Epoch 033: Loss=0.4384, Train Acc=0.8449\n",
      "Epoch 034: Loss=0.4350, Train Acc=0.8458\n",
      "Epoch 035: Loss=0.4318, Train Acc=0.8467\n",
      "Epoch 036: Loss=0.4286, Train Acc=0.8476\n",
      "Epoch 037: Loss=0.4256, Train Acc=0.8485\n",
      "Epoch 038: Loss=0.4226, Train Acc=0.8494\n",
      "Epoch 039: Loss=0.4198, Train Acc=0.8502\n",
      "Epoch 040: Loss=0.4170, Train Acc=0.8511\n",
      "Epoch 041: Loss=0.4143, Train Acc=0.8518\n",
      "Epoch 042: Loss=0.4117, Train Acc=0.8525\n",
      "Epoch 043: Loss=0.4092, Train Acc=0.8531\n",
      "Epoch 044: Loss=0.4068, Train Acc=0.8537\n",
      "Epoch 045: Loss=0.4044, Train Acc=0.8540\n",
      "Epoch 046: Loss=0.4021, Train Acc=0.8543\n",
      "Epoch 047: Loss=0.3999, Train Acc=0.8545\n",
      "Epoch 048: Loss=0.3978, Train Acc=0.8548\n",
      "Epoch 049: Loss=0.3957, Train Acc=0.8549\n",
      "Epoch 050: Loss=0.3937, Train Acc=0.8549\n",
      "Epoch 051: Loss=0.3917, Train Acc=0.8549\n",
      "Epoch 052: Loss=0.3898, Train Acc=0.8549\n",
      "Epoch 053: Loss=0.3879, Train Acc=0.8549\n",
      "Epoch 054: Loss=0.3861, Train Acc=0.8549\n",
      "Epoch 055: Loss=0.3844, Train Acc=0.8548\n",
      "Epoch 056: Loss=0.3826, Train Acc=0.8548\n",
      "Epoch 057: Loss=0.3810, Train Acc=0.8547\n",
      "Epoch 058: Loss=0.3793, Train Acc=0.8547\n",
      "Epoch 059: Loss=0.3777, Train Acc=0.8547\n",
      "Epoch 060: Loss=0.3762, Train Acc=0.8547\n",
      "Epoch 061: Loss=0.3747, Train Acc=0.8547\n",
      "Epoch 062: Loss=0.3732, Train Acc=0.8547\n",
      "Epoch 063: Loss=0.3717, Train Acc=0.8547\n",
      "Epoch 064: Loss=0.3703, Train Acc=0.8547\n",
      "Epoch 065: Loss=0.3690, Train Acc=0.8548\n",
      "Epoch 066: Loss=0.3676, Train Acc=0.8548\n",
      "Epoch 067: Loss=0.3663, Train Acc=0.8549\n",
      "Epoch 068: Loss=0.3650, Train Acc=0.8549\n",
      "Epoch 069: Loss=0.3637, Train Acc=0.8550\n",
      "Epoch 070: Loss=0.3625, Train Acc=0.8552\n",
      "Epoch 071: Loss=0.3613, Train Acc=0.8554\n",
      "Epoch 072: Loss=0.3601, Train Acc=0.8554\n",
      "Epoch 073: Loss=0.3590, Train Acc=0.8557\n",
      "Epoch 074: Loss=0.3578, Train Acc=0.8558\n",
      "Epoch 075: Loss=0.3567, Train Acc=0.8559\n",
      "Epoch 076: Loss=0.3556, Train Acc=0.8560\n",
      "Epoch 077: Loss=0.3545, Train Acc=0.8561\n",
      "Epoch 078: Loss=0.3535, Train Acc=0.8563\n",
      "Epoch 079: Loss=0.3525, Train Acc=0.8565\n",
      "Epoch 080: Loss=0.3514, Train Acc=0.8566\n",
      "Epoch 081: Loss=0.3504, Train Acc=0.8568\n",
      "Epoch 082: Loss=0.3495, Train Acc=0.8569\n",
      "Epoch 083: Loss=0.3485, Train Acc=0.8570\n",
      "Epoch 084: Loss=0.3476, Train Acc=0.8571\n",
      "Epoch 085: Loss=0.3466, Train Acc=0.8573\n",
      "Epoch 086: Loss=0.3457, Train Acc=0.8575\n",
      "Epoch 087: Loss=0.3448, Train Acc=0.8577\n",
      "Epoch 088: Loss=0.3439, Train Acc=0.8578\n",
      "Epoch 089: Loss=0.3431, Train Acc=0.8579\n",
      "Epoch 090: Loss=0.3422, Train Acc=0.8581\n",
      "Epoch 091: Loss=0.3414, Train Acc=0.8583\n",
      "Epoch 092: Loss=0.3405, Train Acc=0.8585\n",
      "Epoch 093: Loss=0.3397, Train Acc=0.8587\n",
      "Epoch 094: Loss=0.3389, Train Acc=0.8589\n",
      "Epoch 095: Loss=0.3381, Train Acc=0.8590\n",
      "Epoch 096: Loss=0.3373, Train Acc=0.8591\n",
      "Epoch 097: Loss=0.3366, Train Acc=0.8592\n",
      "Epoch 098: Loss=0.3358, Train Acc=0.8594\n",
      "Epoch 099: Loss=0.3351, Train Acc=0.8595\n",
      "Epoch 100: Loss=0.3343, Train Acc=0.8596\n",
      "Test Accuracy: 0.9106\n",
      "\n",
      "Sample explanations:\n",
      "Sample 0 (APPROVED, p=0.89): [('loan_type_name_VA-guaranteed', np.float32(-0.2995505)), ('purchaser_type_name_Fannie Mae (FNMA)', np.float32(-0.27765685)), ('purchaser_type_name_Freddie Mac (FHLMC)', np.float32(-0.19947182))]\n",
      "Sample 1 (APPROVED, p=0.95): [('sequence_number', np.float32(-0.3909752)), ('loan_type_name_VA-guaranteed', np.float32(-0.2995505)), ('purchaser_type_name_Fannie Mae (FNMA)', np.float32(-0.27765685))]\n",
      "Sample 2 (APPROVED, p=0.96): [('purchaser_type_name_Ginnie Mae (GNMA)', np.float32(-0.24309349)), ('purchaser_type_name_Freddie Mac (FHLMC)', np.float32(-0.19947182)), ('loan_purpose_name_Home purchase', np.float32(-0.13243541))]\n",
      "Sample 3 (APPROVED, p=0.86): [('loan_type_name_VA-guaranteed', np.float32(-0.2995505)), ('purchaser_type_name_Fannie Mae (FNMA)', np.float32(-0.27765685)), ('purchaser_type_name_Ginnie Mae (GNMA)', np.float32(-0.24309349))]\n",
      "Sample 4 (APPROVED, p=0.92): [('purchaser_type_name_Fannie Mae (FNMA)', np.float32(-0.27765685)), ('purchaser_type_name_Ginnie Mae (GNMA)', np.float32(-0.24309349)), ('purchaser_type_name_Freddie Mac (FHLMC)', np.float32(-0.19947182))]\n",
      "\n",
      "ONNX model saved to logistic_regression_model.onnx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_wa = pd.read_csv(r\"hmda_2016_wa_all-records_labels.csv\", low_memory=False)\n",
    "df_ak = pd.read_csv(r\"hmda_2016_ak_all-records_labels.csv\", low_memory=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n",
    "cols_to_use = [\n",
    "    'loan_type_name', 'loan_purpose_name', 'loan_amount_000s',\n",
    "    'applicant_income_000s', 'property_type_name', 'purchaser_type_name',\n",
    "    'owner_occupancy_name', 'applicant_ethnicity_name', 'preapproval_name',\n",
    "    'lien_status_name', 'sequence_number',\n",
    "    'number_of_owner_occupied_units', 'number_of_1_to_4_family_units',\n",
    "    'hud_median_family_income', 'tract_to_msamd_income',\n",
    "    'applicant_race_name_1', 'applicant_sex_name', 'action_taken'\n",
    "]\n",
    "\n",
    "train_df = df_wa[cols_to_use].copy()\n",
    "test_df = df_ak[cols_to_use].copy()\n",
    "\n",
    "num_cols = [\n",
    "    'applicant_income_000s', 'number_of_owner_occupied_units',\n",
    "    'number_of_1_to_4_family_units', 'hud_median_family_income',\n",
    "    'tract_to_msamd_income'\n",
    "]\n",
    "for col in num_cols:\n",
    "    med_train = train_df[col].median()\n",
    "    med_test = test_df[col].median()\n",
    "    train_df[col] = train_df[col].fillna(med_train)\n",
    "    test_df[col] = test_df[col].fillna(med_test)\n",
    "\n",
    "categorical_cols = [\n",
    "    'loan_type_name', 'loan_purpose_name', 'property_type_name',\n",
    "    'purchaser_type_name', 'owner_occupancy_name',\n",
    "    'applicant_ethnicity_name', 'preapproval_name',\n",
    "    'lien_status_name',\n",
    "    'applicant_race_name_1', 'applicant_sex_name'\n",
    "]\n",
    "numerical_cols = [\n",
    "    'loan_amount_000s', 'applicant_income_000s', 'sequence_number',\n",
    "    'number_of_owner_occupied_units', 'number_of_1_to_4_family_units',\n",
    "    'hud_median_family_income', 'tract_to_msamd_income'\n",
    "]\n",
    "target = 'action_taken'\n",
    "\n",
    "train_ohe = pd.get_dummies(train_df, columns=categorical_cols, drop_first=False)\n",
    "test_ohe = pd.get_dummies(test_df, columns=categorical_cols, drop_first=False)\n",
    "\n",
    "train_ohe, test_ohe = train_ohe.align(test_ohe, join='left', axis=1, fill_value=0)\n",
    "\n",
    "def to_binary(code):\n",
    "    approved = {1,2,6,8}\n",
    "    return 1 if code in approved else 0\n",
    "\n",
    "y_train = train_ohe[target].apply(to_binary).values\n",
    "X_train = train_ohe.drop(columns=[target]).values\n",
    "y_test = test_ohe[target].apply(to_binary).values\n",
    "X_test = test_ohe.drop(columns=[target]).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.reshape(-1,1), dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.reshape(-1,1), dtype=torch.float32).to(device)\n",
    "\n",
    "model = LogisticRegression(X_train_tensor.shape[1]).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    model.train()\n",
    "    preds = model(X_train_tensor)\n",
    "    loss = criterion(preds, y_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        acc = ((preds>=0.5).float()==y_train_tensor).float().mean().item()\n",
    "    print(f\"Epoch {epoch:03d}: Loss={loss.item():.4f}, Train Acc={acc:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prob_test = model(X_test_tensor)\n",
    "    acc_test = ((prob_test>=0.5).float()==y_test_tensor).float().mean().item()\n",
    "    print(f\"Test Accuracy: {acc_test:.4f}\")\n",
    "\n",
    "weights = model.linear.weight.detach().cpu().flatten().numpy()\n",
    "bias = model.linear.bias.detach().item()\n",
    "feature_names = train_ohe.drop(columns=[target]).columns.tolist()\n",
    "\n",
    "def explain_rejection(x, top_k=3, include_bias=False):\n",
    "    x_np = x.cpu().numpy().flatten()\n",
    "    contrib = weights * x_np\n",
    "    names = feature_names.copy()\n",
    "    if include_bias:\n",
    "        contrib = np.append(contrib, bias)\n",
    "        names.append('BIAS')\n",
    "    worst = np.argsort(contrib)[:top_k]\n",
    "    return [(names[i], contrib[i]) for i in worst]\n",
    "\n",
    "print(\"\\nSample explanations:\")\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        p = model(X_test_tensor[i].unsqueeze(0)).item()\n",
    "        label = \"APPROVED\" if p>=0.5 else \"REJECTED\"\n",
    "        print(f\"Sample {i} ({label}, p={p:.2f}):\", explain_rejection(X_test_tensor[i]))\n",
    "\n",
    "model_cpu = model.cpu().eval()\n",
    "dummy_in = torch.randn(1, X_train_tensor.shape[1], dtype=torch.float32)\n",
    "torch.onnx.export(\n",
    "    model_cpu, dummy_in, \"logistic_regression_model.onnx\",\n",
    "    export_params=True, opset_version=11, do_constant_folding=True,\n",
    "    input_names=['input'], output_names=['output'],\n",
    "    dynamic_axes={'input':{0:'batch_size'}, 'output':{0:'batch_size'}}\n",
    ")\n",
    "print(\"\\nONNX model saved to logistic_regression_model.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb6d713-6834-4a6f-b5ae-c2d66f2b2638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020: Loss=0.5006, Train Acc=0.8058\n",
      "Epoch 040: Loss=0.4128, Train Acc=0.8590\n",
      "Epoch 060: Loss=0.3691, Train Acc=0.8576\n",
      "Epoch 080: Loss=0.3439, Train Acc=0.8608\n",
      "Epoch 100: Loss=0.3273, Train Acc=0.8623\n",
      "Test Accuracy: 0.9135\n",
      "✅ Model data saved for DYNAMIC explanations!\n",
      "🚫 No stored explanations - all explanations computed on-demand\n",
      "📊 Features: 52\n",
      "🎯 Background samples for SHAP: 100\n",
      "✅ ONNX model saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import shap\n",
    "import lime.lime_tabular\n",
    "\n",
    "# [Previous data loading and model training code remains the same...]\n",
    "# Load data\n",
    "df_wa = pd.read_csv(r\"hmda_2016_wa_all-records_labels.csv\", low_memory=False)\n",
    "df_ak = pd.read_csv(r\"hmda_2016_ak_all-records_labels.csv\", low_memory=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n",
    "cols_to_use = [\n",
    "    'loan_type_name', 'loan_purpose_name', 'loan_amount_000s',\n",
    "    'applicant_income_000s', 'property_type_name', 'purchaser_type_name',\n",
    "    'owner_occupancy_name', 'applicant_ethnicity_name', 'preapproval_name',\n",
    "    'lien_status_name', 'sequence_number',\n",
    "    'number_of_owner_occupied_units', 'number_of_1_to_4_family_units',\n",
    "    'hud_median_family_income', 'tract_to_msamd_income',\n",
    "    'applicant_race_name_1', 'applicant_sex_name', 'action_taken'\n",
    "]\n",
    "\n",
    "train_df = df_wa[cols_to_use].copy()\n",
    "test_df = df_ak[cols_to_use].copy()\n",
    "\n",
    "# [Data preprocessing code...]\n",
    "num_cols = [\n",
    "    'applicant_income_000s', 'number_of_owner_occupied_units',\n",
    "    'number_of_1_to_4_family_units', 'hud_median_family_income',\n",
    "    'tract_to_msamd_income'\n",
    "]\n",
    "for col in num_cols:\n",
    "    med_train = train_df[col].median()\n",
    "    med_test = test_df[col].median()\n",
    "    train_df[col] = train_df[col].fillna(med_train)\n",
    "    test_df[col] = test_df[col].fillna(med_test)\n",
    "\n",
    "categorical_cols = [\n",
    "    'loan_type_name', 'loan_purpose_name', 'property_type_name',\n",
    "    'purchaser_type_name', 'owner_occupancy_name',\n",
    "    'applicant_ethnicity_name', 'preapproval_name',\n",
    "    'lien_status_name', 'applicant_race_name_1', 'applicant_sex_name'\n",
    "]\n",
    "numerical_cols = [\n",
    "    'loan_amount_000s', 'applicant_income_000s', 'sequence_number',\n",
    "    'number_of_owner_occupied_units', 'number_of_1_to_4_family_units',\n",
    "    'hud_median_family_income', 'tract_to_msamd_income'\n",
    "]\n",
    "target = 'action_taken'\n",
    "\n",
    "train_ohe = pd.get_dummies(train_df, columns=categorical_cols, drop_first=False)\n",
    "test_ohe = pd.get_dummies(test_df, columns=categorical_cols, drop_first=False)\n",
    "train_ohe, test_ohe = train_ohe.align(test_ohe, join='left', axis=1, fill_value=0)\n",
    "\n",
    "def to_binary(code):\n",
    "    approved = {1,2,6,8}\n",
    "    return 1 if code in approved else 0\n",
    "\n",
    "y_train = train_ohe[target].apply(to_binary).values\n",
    "X_train = train_ohe.drop(columns=[target]).values\n",
    "y_test = test_ohe[target].apply(to_binary).values\n",
    "X_test = test_ohe.drop(columns=[target]).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.reshape(-1,1), dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.reshape(-1,1), dtype=torch.float32).to(device)\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(X_train_tensor.shape[1]).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    model.train()\n",
    "    preds = model(X_train_tensor)\n",
    "    loss = criterion(preds, y_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 20 == 0:\n",
    "        with torch.no_grad():\n",
    "            acc = ((preds>=0.5).float()==y_train_tensor).float().mean().item()\n",
    "        print(f\"Epoch {epoch:03d}: Loss={loss.item():.4f}, Train Acc={acc:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prob_test = model(X_test_tensor)\n",
    "    acc_test = ((prob_test>=0.5).float()==y_test_tensor).float().mean().item()\n",
    "    print(f\"Test Accuracy: {acc_test:.4f}\")\n",
    "\n",
    "feature_names = train_ohe.drop(columns=[target]).columns.tolist()\n",
    "\n",
    "# Save model weights and metadata ONLY (no stored explanations)\n",
    "weights = model.linear.weight.detach().cpu().flatten().numpy().tolist()\n",
    "bias = model.linear.bias.detach().cpu().item()\n",
    "scaler_mean = scaler.mean_.tolist()\n",
    "scaler_scale = scaler.scale_.tolist()\n",
    "\n",
    "# Save background data for dynamic SHAP explanations\n",
    "background_data = X_train_scaled[:100].tolist()  # Small background sample\n",
    "\n",
    "# Clean model data - NO STORED EXPLANATIONS\n",
    "model_data = {\n",
    "    \"weights\": weights,\n",
    "    \"bias\": bias,\n",
    "    \"scaler_mean\": scaler_mean,\n",
    "    \"scaler_scale\": scaler_scale,\n",
    "    \"feature_names\": feature_names,\n",
    "    \"background_data\": background_data,  # For dynamic SHAP\n",
    "    \"feature_categories\": {\n",
    "        \"numerical\": numerical_cols,\n",
    "        \"categorical\": categorical_cols\n",
    "    },\n",
    "    \"dynamic_explanations\": True  # Flag indicating dynamic explanations\n",
    "}\n",
    "\n",
    "# Save clean model data (no stored explanations)\n",
    "with open('model_data.json', 'w') as f:\n",
    "    json.dump(model_data, f, indent=2)\n",
    "\n",
    "print(\"✅ Model data saved for DYNAMIC explanations!\")\n",
    "print(\"🚫 No stored explanations - all explanations computed on-demand\")\n",
    "print(f\"📊 Features: {len(feature_names)}\")\n",
    "print(f\"🎯 Background samples for SHAP: {len(background_data)}\")\n",
    "\n",
    "# Export ONNX model\n",
    "model_cpu = model.cpu().eval()\n",
    "dummy_in = torch.randn(1, X_train_tensor.shape[1], dtype=torch.float32)\n",
    "torch.onnx.export(\n",
    "    model_cpu, dummy_in, \"logistic_regression_model.onnx\",\n",
    "    export_params=True, opset_version=11, do_constant_folding=True,\n",
    "    input_names=['input'], output_names=['output'],\n",
    "    dynamic_axes={'input':{0:'batch_size'}, 'output':{0:'batch_size'}}\n",
    ")\n",
    "print(\"✅ ONNX model saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2484fa-7533-4c40-a303-19461d2fb1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
